{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86de6bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tonnu\\anaconda3\\envs\\transform\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import (\n",
    "    MT5ForConditionalGeneration,\n",
    "    Seq2SeqTrainer, MT5Tokenizer, MT5Config\n",
    ")\n",
    "\n",
    "import datasets\n",
    "import pandas as pd\n",
    "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "import gc\n",
    "import datasets\n",
    "import os\n",
    "import torch\n",
    "\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" # see issue #152\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\n",
    "#os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "#!export CUDA_VISIBLE_DEVICES=0\n",
    "#device, use_gpu = (\"cuda:0\", True) if torch.cuda.is_available() else (\"cpu\", False)\n",
    "#device, use_gpu = (\"cpu\", False)  # Force CPU usage\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b2d7230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets\n",
    "# !pip install --upgrade accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ec62d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type t5 to instantiate a model of type mt5. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model done\n",
      "load tokenizer done\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "checkpoint = \"VietAI/vit5-base\"\n",
    "model = MT5ForConditionalGeneration.from_pretrained(checkpoint)\n",
    "model = model.to(device)\n",
    "print('load model done')\n",
    "tokenizer = MT5Tokenizer.from_pretrained(checkpoint)\n",
    "print('load tokenizer done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae074a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['index', 'Original', 'Paraphrase'],\n",
       "    num_rows: 55\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdata = pd.read_excel(\"DATA_chuyen_cau.xlsx\")\n",
    "tdata = tdata.applymap(str)\n",
    "tdata = tdata.reset_index()\n",
    "dataset = datasets.Dataset.from_pandas(tdata)\n",
    "\n",
    "train = dataset.train_test_split(\n",
    "    train_size=0.8, test_size=0.2, shuffle = False\n",
    ")\n",
    "\n",
    "train_data = train['train']\n",
    "test_data = train['test']\n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8a6ff52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [00:00<00:00, 2483.89 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [00:00<00:00, 1291.61 examples/s]\n",
      "Map:   0%|                                                                               | 0/55 [00:00<?, ? examples/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\tonnu\\anaconda3\\envs\\transform\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2304: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [00:00<00:00, 764.50 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [00:00<00:00, 945.36 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def format_dataset(example):\n",
    "     return {'input': example['Original'], 'target': example['Paraphrase']}\n",
    "train_data = train_data.map(format_dataset, remove_columns=train_data.column_names)\n",
    "test_data = test_data.map(format_dataset, remove_columns=test_data.column_names)\n",
    "\n",
    "def convert_to_features(example_batch):\n",
    "    input_encodings = tokenizer.batch_encode_plus(example_batch['input'], pad_to_max_length=True, max_length=128)\n",
    "    target_encodings = tokenizer.batch_encode_plus(example_batch['target'], pad_to_max_length=True, max_length=128)\n",
    "    encodings = {\n",
    "        'input_ids': input_encodings['input_ids'], \n",
    "        'attention_mask': input_encodings['attention_mask'],\n",
    "        'labels': target_encodings['input_ids'],\n",
    "        'decoder_attention_mask': target_encodings['attention_mask']\n",
    "    }\n",
    "\n",
    "    return encodings\n",
    "train_data = train_data.map(convert_to_features, batched=True, remove_columns=train_data.column_names)\n",
    "test_data = test_data.map(convert_to_features, batched=True, remove_columns=test_data.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1717efee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be60357a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tonnu\\AppData\\Local\\Temp\\ipykernel_23232\\3190919124.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge = load_metric(\"rouge\")\n",
      "C:\\Users\\tonnu\\anaconda3\\envs\\transform\\lib\\site-packages\\datasets\\load.py:756: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/rouge/rouge.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "rouge = load_metric(\"rouge\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    # all unnecessary tokens are removed\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "\n",
    "    return {\n",
    "        \"rouge2_precision\": round(rouge_output.precision, 4),\n",
    "        \"rouge2_recall\": round(rouge_output.recall, 4),\n",
    "        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a2e3236",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Set device to CPU\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Set CUDA_VISIBLE_DEVICES environment variable to use CPU only\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3d1c8f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "num_train_epochs = 5\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer,model=model)\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"viT5-base-1\",\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_eval_batch_size=1,\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    logging_steps=22859,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=45718,\n",
    "    eval_steps=22859,\n",
    "    overwrite_output_dir=True,\n",
    "    save_total_limit=4,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=None,\n",
    "    group_by_length=True,\n",
    "    #fp16=True, \n",
    ")\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    data_collator = data_collator,\n",
    "    tokenizer = tokenizer,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "09133add",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 55\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 275\n",
      "  Number of trainable parameters = 253672704\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='275' max='275' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [275/275 23:12, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=275, training_loss=0.40245508367365057, metrics={'train_runtime': 1399.6033, 'train_samples_per_second': 0.196, 'train_steps_per_second': 0.196, 'total_flos': 47720683929600.0, 'train_loss': 0.40245508367365057, 'epoch': 5.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5da9b62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: M·ªôt c·ª≠a h√†ng c√≥ m√£ c·ª≠a h√†ng ƒë·ªÉ ph√¢n bi·ªát v·ªõi c√°c c·ª≠a h√†ng kh√°c, c√≥ t√™n c·ª≠a h√†ng, ƒë·ªãa ch·ªâ, t√™n ng∆∞·ªùi qu·∫£n l√Ω.\n",
      "Output: M·ªói c·ª≠a h√†ng ƒë∆∞·ª£c x√°c ƒë·ªãnh b·ªüi duy nh·∫•t m·ªôt m√£ c·ª≠a h√†ng, t√™n c·ª≠a h√†ng, ƒë·ªãa ch·ªâ, t√™n ng∆∞·ªùi qu·∫£n l√Ω.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input: M·ªôt quy·ªÉn s√°ch c√≥ c√°c th√¥ng tin g·ªìm m√£ s√°ch, t√™n s√°ch, t√™n t√°c gi·∫£.\n",
      "Output: M·ªói quy·ªÉn s√°ch c√≥ nhi·ªÅu th√¥ng tin.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# run\n",
    "def paraphase(text):\n",
    "    inputs = tokenizer(text, padding='longest', max_length=64, return_tensors='pt')\n",
    "    input_ids = inputs.input_ids\n",
    "    attention_mask = inputs.attention_mask\n",
    "    output = model.generate(input_ids, attention_mask=attention_mask, max_length=64)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "texts = [\n",
    "        \"M·ªôt c·ª≠a h√†ng c√≥ m√£ c·ª≠a h√†ng ƒë·ªÉ ph√¢n bi·ªát v·ªõi c√°c c·ª≠a h√†ng kh√°c, c√≥ t√™n c·ª≠a h√†ng, ƒë·ªãa ch·ªâ, t√™n ng∆∞·ªùi qu·∫£n l√Ω.\",\n",
    "        \"M·ªôt quy·ªÉn s√°ch c√≥ c√°c th√¥ng tin g·ªìm m√£ s√°ch, t√™n s√°ch, t√™n t√°c gi·∫£.\"\n",
    "        ]\n",
    "for text in texts:\n",
    "    print(f'Input: {text}')\n",
    "    print(f'Output: {paraphase(text)}')\n",
    "    print('-'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c93ce746-ac61-4310-8e64-8434ae80432c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: M·ªôt c·ª≠a h√†ng c√≥ m√£ c·ª≠a h√†ng ƒë·ªÉ ph√¢n bi·ªát v·ªõi c√°c c·ª≠a h√†ng kh√°c, c√≥ t√™n c·ª≠a h√†ng, ƒë·ªãa ch·ªâ, t√™n ng∆∞·ªùi qu·∫£n l√Ω.\n",
      "Output 0: M·ªói c·ª≠a h√†ng ƒë∆∞·ª£c x√°c ƒë·ªãnh b·ªüi duy nh·∫•t m·ªôt m√£ c·ª≠a h√†ng, t√™n c·ª≠a h√†ng, ƒë·ªãa ch·ªâ, ƒë·ªãa ch·ªâ, t√™n ng∆∞·ªùi qu·∫£n l√Ω.\n",
      "Output 1: M·ªói c·ª≠a h√†ng ƒë∆∞·ª£c x√°c ƒë·ªãnh b·ªüi duy nh·∫•t m·ªôt m√£ c·ª≠a h√†ng, t√™n c·ª≠a h√†ng, ƒë·ªãa ch·ªâ, t√™n ng∆∞·ªùi qu·∫£n l√Ω.\n",
      "Output 2: M·ªói c·ª≠a h√†ng ƒë∆∞·ª£c x√°c ƒë·ªãnh b·ªüi duy nh·∫•t m·ªôt m√£ c·ª≠a h√†ng, t√™n c·ª≠a h√†ng, ƒë·ªãa ch·ªâ, s·ªë ng∆∞·ªùi qu·∫£n l√Ω.\n",
      "Output 3: M·ªói c·ª≠a h√†ng ƒë∆∞·ª£c x√°c ƒë·ªãnh b·ªüi duy nh·∫•t m·ªôt m√£ c·ª≠a h√†ng, t√™n c·ª≠a h√†ng, ƒë·ªãa ch·ªâ, s·ªë c·ª≠a h√†ng.\n",
      "Output 4: M·ªói c·ª≠a h√†ng c√≥ nhi·ªÅu c·ª≠a h√†ng.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input: M·ªôt quy·ªÉn s√°ch c√≥ c√°c th√¥ng tin g·ªìm m√£ s√°ch, t√™n s√°ch, t√™n t√°c gi·∫£.\n",
      "Output 0: M·ªôt quy·ªÉn s√°ch c√≥ nhi·ªÅu th√¥ng tin. M·ªói th√¥ng tin ƒë∆∞·ª£c x√°c ƒë·ªãnh b·ªüi duy nh·∫•t m·ªôt m√£ s√°ch, t√™n s√°ch, t√™n t√°c gi·∫£.\n",
      "Output 1: M·ªôt quy·ªÉn s√°ch c√≥ nhi·ªÅu th√¥ng tin. M·ªói th√¥ng tin c√≥ nhi·ªÅu s√°ch.\n",
      "Output 2: M·ªôt quy·ªÉn s√°ch c√≥ nhi·ªÅu th√¥ng tin. M·ªói th√¥ng tin c√≥ nhi·ªÅu s√°ch, t√™n s√°ch, t√™n t√°c gi·∫£.\n",
      "Output 3: M·ªói quy·ªÉn s√°ch ƒë∆∞·ª£c x√°c ƒë·ªãnh b·ªüi duy nh·∫•t m·ªôt m√£ s√°ch, t√™n s√°ch, t√™n t√°c gi·∫£.\n",
      "Output 4: M·ªôt quy·ªÉn s√°ch c√≥ nhi·ªÅu th√¥ng tin. M·ªói th√¥ng tin c√≥ nhi·ªÅu lo·∫°i th√¥ng tin.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def paraphase_5_(text):\n",
    "    inputs = tokenizer(text, padding='longest', max_length=64, return_tensors='pt')\n",
    "    input_ids = inputs.input_ids\n",
    "    attention_mask = inputs.attention_mask\n",
    "    outputs = model.generate(input_ids, attention_mask=attention_mask, max_length=64,num_beams=5, num_return_sequences=5)\n",
    "    for i, output in enumerate(outputs):\n",
    "        print(f'Output {i}: {tokenizer.decode(output, skip_special_tokens=True)}')\n",
    "    # return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "texts = [\n",
    "        \"M·ªôt c·ª≠a h√†ng c√≥ m√£ c·ª≠a h√†ng ƒë·ªÉ ph√¢n bi·ªát v·ªõi c√°c c·ª≠a h√†ng kh√°c, c√≥ t√™n c·ª≠a h√†ng, ƒë·ªãa ch·ªâ, t√™n ng∆∞·ªùi qu·∫£n l√Ω.\",\n",
    "        \"M·ªôt quy·ªÉn s√°ch c√≥ c√°c th√¥ng tin g·ªìm m√£ s√°ch, t√™n s√°ch, t√™n t√°c gi·∫£.\"\n",
    "        ]\n",
    "for text in texts:\n",
    "    print(f'Input: {text}')\n",
    "    paraphase_5_(text)\n",
    "    print('-'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaa8ca9-70d3-4062-bdde-46073f1b694d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transform",
   "language": "python",
   "name": "transform"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
