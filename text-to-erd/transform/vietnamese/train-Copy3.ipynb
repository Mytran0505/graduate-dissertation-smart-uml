{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86de6bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tonnu\\anaconda3\\envs\\transform\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import (\n",
    "    MT5ForConditionalGeneration,\n",
    "    Seq2SeqTrainer, MT5Tokenizer, MT5Config\n",
    ")\n",
    "\n",
    "import datasets\n",
    "import pandas as pd\n",
    "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "import gc\n",
    "import datasets\n",
    "import os\n",
    "import torch\n",
    "\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" # see issue #152\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\n",
    "#os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "#!export CUDA_VISIBLE_DEVICES=0\n",
    "#device, use_gpu = (\"cuda:0\", True) if torch.cuda.is_available() else (\"cpu\", False)\n",
    "#device, use_gpu = (\"cpu\", False)  # Force CPU usage\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b2d7230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets\n",
    "# !pip install --upgrade accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ec62d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type t5 to instantiate a model of type mt5. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model done\n",
      "load tokenizer done\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "checkpoint = \"VietAI/vit5-base\"\n",
    "model = MT5ForConditionalGeneration.from_pretrained(checkpoint)\n",
    "model = model.to(device)\n",
    "print('load model done')\n",
    "tokenizer = MT5Tokenizer.from_pretrained(checkpoint)\n",
    "print('load tokenizer done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae074a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['index', 'Original', 'Paraphrase'],\n",
       "    num_rows: 200\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdata = pd.read_excel(\"DATA_chuyen_cau_khong_dau.xlsx\")\n",
    "tdata = tdata.applymap(str)\n",
    "tdata = tdata.reset_index()\n",
    "dataset = datasets.Dataset.from_pandas(tdata)\n",
    "\n",
    "train = dataset.train_test_split(\n",
    "    train_size=0.8, test_size=0.2, shuffle = False\n",
    ")\n",
    "\n",
    "train_data = train['train']\n",
    "test_data = train['test']\n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8a6ff52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 3426.70 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 4481.67 examples/s]\n",
      "Map:   0%|                                                                              | 0/200 [00:00<?, ? examples/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\tonnu\\anaconda3\\envs\\transform\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2304: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 1234.56 examples/s]\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 988.79 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def format_dataset(example):\n",
    "     return {'input': example['Original'], 'target': example['Paraphrase']}\n",
    "train_data = train_data.map(format_dataset, remove_columns=train_data.column_names)\n",
    "test_data = test_data.map(format_dataset, remove_columns=test_data.column_names)\n",
    "\n",
    "def convert_to_features(example_batch):\n",
    "    input_encodings = tokenizer.batch_encode_plus(example_batch['input'], pad_to_max_length=True, max_length=128)\n",
    "    target_encodings = tokenizer.batch_encode_plus(example_batch['target'], pad_to_max_length=True, max_length=128)\n",
    "    encodings = {\n",
    "        'input_ids': input_encodings['input_ids'], \n",
    "        'attention_mask': input_encodings['attention_mask'],\n",
    "        'labels': target_encodings['input_ids'],\n",
    "        'decoder_attention_mask': target_encodings['attention_mask']\n",
    "    }\n",
    "\n",
    "    return encodings\n",
    "train_data = train_data.map(convert_to_features, batched=True, remove_columns=train_data.column_names)\n",
    "test_data = test_data.map(convert_to_features, batched=True, remove_columns=test_data.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1717efee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be60357a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tonnu\\AppData\\Local\\Temp\\ipykernel_7200\\3190919124.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge = load_metric(\"rouge\")\n",
      "C:\\Users\\tonnu\\anaconda3\\envs\\transform\\lib\\site-packages\\datasets\\load.py:756: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/rouge/rouge.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "rouge = load_metric(\"rouge\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    # all unnecessary tokens are removed\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "\n",
    "    return {\n",
    "        \"rouge2_precision\": round(rouge_output.precision, 4),\n",
    "        \"rouge2_recall\": round(rouge_output.recall, 4),\n",
    "        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b972feb8-070e-4efd-bcaf-a29bd0da6f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Set device to CPU\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Set CUDA_VISIBLE_DEVICES environment variable to use CPU only\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d1c8f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epochs = 5\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer,model=model)\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"viT5-base-1\",\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_eval_batch_size=1,\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    logging_steps=22859,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=45718,\n",
    "    eval_steps=22859,\n",
    "    overwrite_output_dir=True,\n",
    "    save_total_limit=4,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=None,\n",
    "    group_by_length=True,\n",
    "    #fp16=True, \n",
    ")\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    data_collator = data_collator,\n",
    "    tokenizer = tokenizer,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09133add",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tonnu\\anaconda3\\envs\\transform\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 200\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1000\n",
      "  Number of trainable parameters = 253672704\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 1:53:51, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=5.22593505859375, metrics={'train_runtime': 6841.3634, 'train_samples_per_second': 0.146, 'train_steps_per_second': 0.146, 'total_flos': 173529759744000.0, 'train_loss': 5.22593505859375, 'epoch': 5.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5da9b62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Một cửa hàng có mã cửa hàng để phân biệt với các cửa hàng khác, có tên cửa hàng, địa chỉ, tên người quản lý.\n",
      "Output: Một cửa hàng được xác định bởi duy nhất một mã cửa hàng, tên cửa hàng, địa chỉ, tên người quản lý.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input: Một quyển sách có các thông tin gồm mã sách, tên sách, tên tác giả.\n",
      "Output: Mỗi quyển sách được xác định bởi duy nhất một mã sách, tên sách, tên tác giả.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# run\n",
    "def paraphase(text):\n",
    "    inputs = tokenizer(text, padding='longest', max_length=64, return_tensors='pt')\n",
    "    input_ids = inputs.input_ids\n",
    "    attention_mask = inputs.attention_mask\n",
    "    output = model.generate(input_ids, attention_mask=attention_mask, max_length=64)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "texts = [\n",
    "        \"Một cửa hàng có mã cửa hàng để phân biệt với các cửa hàng khác, có tên cửa hàng, địa chỉ, tên người quản lý.\",\n",
    "        \"Một quyển sách có các thông tin gồm mã sách, tên sách, tên tác giả.\"\n",
    "        ]\n",
    "for text in texts:\n",
    "    print(f'Input: {text}')\n",
    "    print(f'Output: {paraphase(text)}')\n",
    "    print('-'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c93ce746-ac61-4310-8e64-8434ae80432c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Một cửa hàng có mã cửa hàng để phân biệt với các cửa hàng khác, có tên cửa hàng, địa chỉ, tên người quản lý.\n",
      "Output 0: Một cửa hàng được xác định bởi duy nhất một mã cửa hàng, tên cửa hàng, địa chỉ, tên người quản lý.\n",
      "Output 1: Mỗi cửa hàng được xác định bởi duy nhất một mã cửa hàng, tên cửa hàng, địa chỉ, tên người quản lý.\n",
      "Output 2: Một cửa hàng được xác định bởi duy nhất một mã cửa hàng, tên cửa hàng, địa chỉ, tên người quản lý, cái tên cửa hàng, địa chỉ, tên người quản lý.\n",
      "Output 3: Một cửa hàng được xác định bởi duy nhất một mã cửa hàng, tên cửa hàng, địa chỉ, tên người quản lý, một cửa hàng, tên cửa hàng, địa chỉ, tên người quản lý.\n",
      "Output 4: Mỗi cửa hàng được xác định bởi duy nhất một mã cửa hàng, tên cửa hàng, địa chỉ, tên cửa hàng, địa chỉ, tên người quản lý.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input: Một quyển sách có các thông tin gồm mã sách, tên sách, tên tác giả.\n",
      "Output 0: Mỗi quyển sách được xác định bởi duy nhất một mã sách, tên sách, tên tác giả.\n",
      "Output 1: Một quyển sách có nhiều thông tin. Mỗi quyển sách được xác định bởi duy nhất một mã sách, tên tác giả.\n",
      "Output 2: Một quyển sách có nhiều thông tin. Mỗi thông tin có thể được Một quyển sách có thể được xác định bởi duy nhất một quyển sách có nhiều thông tin.\n",
      "Output 3: Một quyển sách có nhiều thông tin. Mỗi thông tin có thể được Một quyển sách có thể được xác định bởi duy nhất một quyển sách có thể được xác định bởi duy nhất một mã sách\n",
      "Output 4: Một quyển sách có nhiều thông tin. Mỗi thông tin được xác định bởi duy nhất một mã sách, tên tác giả.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def paraphase_5_(text):\n",
    "    inputs = tokenizer(text, padding='longest', max_length=64, return_tensors='pt')\n",
    "    input_ids = inputs.input_ids\n",
    "    attention_mask = inputs.attention_mask\n",
    "    outputs = model.generate(input_ids, attention_mask=attention_mask, max_length=64,num_beams=5, num_return_sequences=5)\n",
    "    for i, output in enumerate(outputs):\n",
    "        print(f'Output {i}: {tokenizer.decode(output, skip_special_tokens=True)}')\n",
    "    # return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "texts = [\n",
    "        \"Một cửa hàng có mã cửa hàng để phân biệt với các cửa hàng khác, có tên cửa hàng, địa chỉ, tên người quản lý.\",\n",
    "        \"Một quyển sách có các thông tin gồm mã sách, tên sách, tên tác giả.\"\n",
    "        ]\n",
    "for text in texts:\n",
    "    print(f'Input: {text}')\n",
    "    paraphase_5_(text)\n",
    "    print('-'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afaa8ca9-70d3-4062-bdde-46073f1b694d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in transform_30\\config.json\n",
      "Model weights saved in transform_30\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained('transform_30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05a62646-26a6-4258-86fd-3ebc877d976f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in transform_30\\tokenizer_config.json\n",
      "Special tokens file saved in transform_30\\special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('transform_30\\\\tokenizer_config.json',\n",
       " 'transform_30\\\\special_tokens_map.json',\n",
       " 'transform_30\\\\spiece.model',\n",
       " 'transform_30\\\\added_tokens.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('transform_30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7664cd81-4b46-4d0e-9d7e-4a854a0e0a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'transform_model_30.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596aa962-16d4-488b-bb54-1c7dc572475f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transform",
   "language": "python",
   "name": "transform"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
