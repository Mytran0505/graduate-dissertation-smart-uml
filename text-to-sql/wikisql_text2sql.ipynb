{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-02T06:10:48.583179Z","iopub.status.busy":"2024-05-02T06:10:48.582752Z","iopub.status.idle":"2024-05-02T06:10:48.638104Z","shell.execute_reply":"2024-05-02T06:10:48.636945Z","shell.execute_reply.started":"2024-05-02T06:10:48.583141Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\20520\\anaconda3\\envs\\py310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import torch\n","from transformers import T5Tokenizer, T5ForConditionalGeneration, AdamW\n","from torch.utils.data import DataLoader, Dataset"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-03-17T12:11:00.769741Z","iopub.status.busy":"2023-03-17T12:11:00.769353Z","iopub.status.idle":"2023-03-17T12:11:00.827471Z","shell.execute_reply":"2023-03-17T12:11:00.826488Z","shell.execute_reply.started":"2023-03-17T12:11:00.769703Z"},"trusted":true},"outputs":[],"source":["df2 = pd.read_csv('data/wikisql/wikisql_schema_train.csv')\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-03-17T12:11:00.839598Z","iopub.status.busy":"2023-03-17T12:11:00.838776Z","iopub.status.idle":"2023-03-17T12:11:00.845808Z","shell.execute_reply":"2023-03-17T12:11:00.844651Z","shell.execute_reply.started":"2023-03-17T12:11:00.83956Z"},"trusted":true},"outputs":[],"source":["questions = df2['question'].tolist()\n","sql_queries = df2['sql'].tolist()"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-03-17T12:11:00.848753Z","iopub.status.busy":"2023-03-17T12:11:00.847818Z","iopub.status.idle":"2023-03-17T12:11:01.051063Z","shell.execute_reply":"2023-03-17T12:11:01.050017Z","shell.execute_reply.started":"2023-03-17T12:11:00.84871Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"]}],"source":["tokenizer = T5Tokenizer.from_pretrained('t5-base')\n","tokenized_inputs = tokenizer.batch_encode_plus(questions, padding=True, truncation=True, return_tensors='pt')\n","tokenized_outputs = tokenizer.batch_encode_plus(sql_queries, padding=True, truncation=True, return_tensors='pt')"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-03-17T12:11:01.052997Z","iopub.status.busy":"2023-03-17T12:11:01.052633Z","iopub.status.idle":"2023-03-17T12:11:01.06133Z","shell.execute_reply":"2023-03-17T12:11:01.059121Z","shell.execute_reply.started":"2023-03-17T12:11:01.052962Z"},"trusted":true},"outputs":[],"source":["class SQLOnlineDataset(Dataset):\n","    def __init__(self, tokenized_inputs, tokenized_outputs):\n","        self.input_ids = tokenized_inputs['input_ids']\n","        self.attention_mask = tokenized_inputs['attention_mask']\n","        self.labels = tokenized_outputs['input_ids']\n","        self.decoder_attention_mask = tokenized_outputs['attention_mask']\n","\n","    def __len__(self):\n","        return len(self.input_ids)\n","\n","    def __getitem__(self, idx):\n","        return {\n","            'input_ids': self.input_ids[idx],\n","            'attention_mask': self.attention_mask[idx],\n","            'labels': self.labels[idx],\n","            'decoder_attention_mask': self.decoder_attention_mask[idx]\n","        }\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-03-17T12:11:01.063752Z","iopub.status.busy":"2023-03-17T12:11:01.063316Z","iopub.status.idle":"2023-03-17T12:11:01.073628Z","shell.execute_reply":"2023-03-17T12:11:01.072255Z","shell.execute_reply.started":"2023-03-17T12:11:01.063705Z"},"trusted":true},"outputs":[],"source":["train_dataset = SQLOnlineDataset(tokenized_inputs, tokenized_outputs)\n","train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-03-17T12:11:01.075875Z","iopub.status.busy":"2023-03-17T12:11:01.075279Z","iopub.status.idle":"2023-03-17T12:11:05.398877Z","shell.execute_reply":"2023-03-17T12:11:05.397718Z","shell.execute_reply.started":"2023-03-17T12:11:01.075837Z"},"trusted":true},"outputs":[],"source":["device = torch.device('cpu')\n","model = T5ForConditionalGeneration.from_pretrained('t5-base').to(device)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-03-17T12:11:05.403589Z","iopub.status.busy":"2023-03-17T12:11:05.402622Z","iopub.status.idle":"2023-03-17T12:11:05.415607Z","shell.execute_reply":"2023-03-17T12:11:05.414364Z","shell.execute_reply.started":"2023-03-17T12:11:05.40355Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\20520\\anaconda3\\envs\\py310\\lib\\site-packages\\transformers\\optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}],"source":["optimizer = AdamW(model.parameters(), lr=3e-5)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-03-17T12:11:05.419144Z","iopub.status.busy":"2023-03-17T12:11:05.418666Z","iopub.status.idle":"2023-03-17T12:11:49.105181Z","shell.execute_reply":"2023-03-17T12:11:49.103928Z","shell.execute_reply.started":"2023-03-17T12:11:05.419103Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 1, Loss: 2.175539255142212\n","Epoch: 2, Loss: 1.5234440565109253\n","Epoch: 3, Loss: 0.6547583937644958\n","Epoch: 4, Loss: 0.7873156070709229\n","Epoch: 5, Loss: 0.4106217324733734\n","Epoch: 6, Loss: 0.27955135703086853\n","Epoch: 7, Loss: 0.1688154935836792\n","Epoch: 8, Loss: 0.029750453308224678\n","Epoch: 9, Loss: 0.1447049379348755\n","Epoch: 10, Loss: 0.01725548692047596\n"]}],"source":["\n","num_epochs = 10\n","for epoch in range(num_epochs):\n","    for batch in train_loader:\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","        decoder_attention_mask = batch['decoder_attention_mask'].to(device)\n","        optimizer.zero_grad()\n","        outputs = model(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            labels=labels,\n","            decoder_attention_mask=decoder_attention_mask,\n","            return_dict=True\n","        )\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","    print(f'Epoch: {epoch+1}, Loss: {loss.item()}')"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-03-17T12:11:49.107406Z","iopub.status.busy":"2023-03-17T12:11:49.106732Z","iopub.status.idle":"2023-03-17T12:11:51.070819Z","shell.execute_reply":"2023-03-17T12:11:51.069294Z","shell.execute_reply.started":"2023-03-17T12:11:49.107366Z"},"trusted":true},"outputs":[{"data":{"text/plain":["('sql_t5_tokenizer\\\\tokenizer_config.json',\n"," 'sql_t5_tokenizer\\\\special_tokens_map.json',\n"," 'sql_t5_tokenizer\\\\spiece.model',\n"," 'sql_t5_tokenizer\\\\added_tokens.json')"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["model.save_pretrained('model/wikisql/sql_t5_model')\n","tokenizer.save_pretrained('model/wikisql/sql_t5_tokenizer')"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-03-17T12:11:51.082745Z","iopub.status.busy":"2023-03-17T12:11:51.079866Z","iopub.status.idle":"2023-03-17T12:12:00.217498Z","shell.execute_reply":"2023-03-17T12:12:00.20826Z","shell.execute_reply.started":"2023-03-17T12:11:51.08268Z"},"trusted":true},"outputs":[],"source":["torch.save(model.state_dict(), 'model/wikisql/sql_model.pt')"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-03-17T12:12:00.227167Z","iopub.status.busy":"2023-03-17T12:12:00.225103Z","iopub.status.idle":"2023-03-17T12:12:04.664068Z","shell.execute_reply":"2023-03-17T12:12:04.662894Z","shell.execute_reply.started":"2023-03-17T12:12:00.227119Z"},"trusted":true},"outputs":[{"data":{"text/plain":["T5ForConditionalGeneration(\n","  (shared): Embedding(32128, 768)\n","  (encoder): T5Stack(\n","    (embed_tokens): Embedding(32128, 768)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","              (relative_attention_bias): Embedding(32, 12)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseActDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): ReLU()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1-11): 11 x T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseActDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): ReLU()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (decoder): T5Stack(\n","    (embed_tokens): Embedding(32128, 768)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","              (relative_attention_bias): Embedding(32, 12)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseActDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): ReLU()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1-11): 11 x T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseActDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): ReLU()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",")"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["model = T5ForConditionalGeneration.from_pretrained('t5-base')\n","model.load_state_dict(torch.load('model/wikisql/sql_model.pt'))\n","model.eval()"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-03-17T12:12:04.666628Z","iopub.status.busy":"2023-03-17T12:12:04.665759Z","iopub.status.idle":"2023-03-17T12:12:04.845268Z","shell.execute_reply":"2023-03-17T12:12:04.84426Z","shell.execute_reply.started":"2023-03-17T12:12:04.666587Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["tokenizer = T5Tokenizer.from_pretrained('t5-base')"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-03-17T12:14:50.527997Z","iopub.status.busy":"2023-03-17T12:14:50.526954Z","iopub.status.idle":"2023-03-17T12:14:53.493809Z","shell.execute_reply":"2023-03-17T12:14:53.492646Z","shell.execute_reply.started":"2023-03-17T12:14:50.527929Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Question: What team has more than 49 laps and a grid of 8?\n","Generated SQL query: SELECT Team FROM table WHERE Grid = 8\n"]}],"source":["new_question = \"What team has more than 49 laps and a grid of 8?\"\n","input_ids = tokenizer.encode(new_question, return_tensors='pt')\n","outputs = model.generate(input_ids=input_ids, max_length=100, num_beams=5, early_stopping=True)\n","sql_query = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","print(f\"Question: {new_question}\")\n","print(f\"Generated SQL query: {sql_query}\")"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":1456187,"sourceId":2409983,"sourceType":"datasetVersion"}],"dockerImageVersionId":30407,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
