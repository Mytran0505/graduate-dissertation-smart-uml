{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1878727,"sourceType":"datasetVersion","datasetId":1118439},{"sourceId":2409983,"sourceType":"datasetVersion","datasetId":1456187}],"dockerImageVersionId":30096,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<center><h1 style=\"font-size:300%; background-color:skyblue; color:grey; padding:60px; font-family:Garamond;\"><b>MACHINE TRANSLATION Using Seq2Seq Modelling</b></h1></center>","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"<center><h1 style=\"font-size:200%; background-color:white; color:lightwhite; padding:15px; font-family:Garamond;\">“If you talk to a man in a language he understands, that goes to his head. If you talk to him in his own language, that goes to his heart.”<br> – <b>Nelson Mandela</b></h1></center>","metadata":{}},{"cell_type":"markdown","source":"<img src='https://miro.medium.com/max/875/1*WWXJ0w6YByfPA9KKmDx2Ug.jpeg' style=\"width:1200px;height:600px;\">","metadata":{}},{"cell_type":"markdown","source":"<center><h1 style=\"font-size:200%; background-color:skyblue; color:black; padding:15px; font-family:Garamond;\"><b>Intro to Notebook</b></h1></center>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-size:150%;\">The objective is to Explain How Seq 2 Seq and LSTMs are used for Machine Translations using an example dataset of converting a German sentence to its English counterpart.</p>\n\n<h1> What is Seq2Seq Modelling ?</h1>\n\n<ul>\n    <li style=\"font-size:150%;\">Sequence-to-sequence learning (Seq2Seq) is about training models to convert sequences from one domain (e.g. sentences in English) to sequences in another domain (e.g. the same sentences translated to German). Our aim is to translate given sentences from German language to English.</li>\n    <li style=\"font-size:150%;\">Sequence-to-Sequence (seq2seq) models are used for a variety of NLP tasks, such as text summarization, speech recognition, DNA sequence modeling, among others.</li>\n    <li style=\"font-size:150%;\">Here, both the input and output are sentences. In other words, these sentences are a sequence of words going in and out of a model. This is the basic idea of Sequence-to-Sequence modeling. The figure below tries to explain this method.</li>\n</ul>\n\n<center><img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2019/01/enc_dec_simple.png\"></center>\n\n<h2>  Here's how it works:</h2>\n\n<ul>\n    <li style=\"font-size:150%;\">Feed the embedding vectors for source sequences (German), to the encoder network, one word at a time.</li>\n    <li style=\"font-size:150%;\">Encode the input sentences into fixed dimension state vectors. At this step, we get the hidden and cell states from the encoder LSTM, and feed it to the decoder LSTM.</li>\n    <li style=\"font-size:150%;\">These states are regarded as initial states by decoder. Additionally, it also has the embedding vectors for target words (English).</li>\n    <li style=\"font-size:150%;\">Decode and output the translated sentence, one word at a time. In this step, the output of the decoder is sent to a softmax layer over the entire target vocabulary.</li>\n</ul>\n\n<h1> What is LSTM ?</h1>\n\n<ul>\n    <li style=\"font-size:150%;\">Long Short-Term Memory (LSTM) networks are a modified version of recurrent neural networks, which makes it easier to remember past data in memory. The vanishing gradient problem of RNN is resolved here. LSTM is well-suited to classify, process and predict time series given time lags of unknown duration. It trains the model by using back-propagation. In an LSTM network, three gates are present:</li>\n</ul>\n\n<center><img src=\"https://miro.medium.com/max/700/1*MwU5yk8f9d6IcLybvGgNxA.jpeg\"></center>\n\n<ul>\n    <li style=\"font-size:150%;\"><b>Input gate —</b> discover which value from input should be used to modify the memory. Sigmoid function decides which values to let through 0,1. and tanh function gives weightage to the values which are passed deciding their level of importance ranging from-1 to 1.</li>\n    <center><img src=\"https://miro.medium.com/max/500/1*k1lxwjsxxn8O4BEiVlQNdg.png\"></center>\n    <li style=\"font-size:150%;\"><b>Forget gate —</b> discover what details to be discarded from the block. It is decided by the sigmoid function. it looks at the previous state(ht-1) and the content input(Xt) and outputs a number between 0(omit this)and 1(keep this)for each number in the cell state Ct−1.</li>\n    <center><img src=\"https://miro.medium.com/max/500/1*bQnecA5sy_eepNkL8I-95A.png\"></center>\n    <li style=\"font-size:150%;\"><b>Output gate —</b> the input and the memory of the block is used to decide the output. Sigmoid function decides which values to let through 0,1. and tanh function gives weightage to the values which are passed deciding their level of importance ranging from-1 to 1 and multiplied with output of Sigmoid.</li>\n    <center><img src=\"https://miro.medium.com/max/700/1*s8532P11PgGi2sZqikZ2kA.png\"></center>\n</ul>","metadata":{}},{"cell_type":"markdown","source":"<center><h1 style=\"font-size:200%; background-color:skyblue; color:black; padding:15px; font-family:Garamond;\"><b>Let's start the Implementation</b></h1></center>","metadata":{}},{"cell_type":"markdown","source":"<h1>Import the Required Libraries</h1>","metadata":{}},{"cell_type":"code","source":"import string\nimport re\nfrom numpy import array, argmax, random, take\nimport pandas as pd\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Embedding, RepeatVector\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import load_model\nfrom keras import optimizers\nimport matplotlib.pyplot as plt\n%matplotlib inline\npd.set_option('display.max_colwidth', 200)","metadata":{"execution":{"iopub.status.busy":"2022-06-10T23:11:05.761332Z","iopub.execute_input":"2022-06-10T23:11:05.761832Z","iopub.status.idle":"2022-06-10T23:11:05.775596Z","shell.execute_reply.started":"2022-06-10T23:11:05.761795Z","shell.execute_reply":"2022-06-10T23:11:05.77402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1> Load the Data </h1>","metadata":{}},{"cell_type":"code","source":"# function to read raw text file\ndef read_text(filename):\n        # open the file\n        file = open(filename, mode='rt', encoding='utf-8')\n        \n        # read all text\n        text = file.read()\n        file.close()\n        return text","metadata":{"execution":{"iopub.status.busy":"2022-06-10T23:11:05.777982Z","iopub.execute_input":"2022-06-10T23:11:05.778321Z","iopub.status.idle":"2022-06-10T23:11:05.787034Z","shell.execute_reply.started":"2022-06-10T23:11:05.778287Z","shell.execute_reply":"2022-06-10T23:11:05.786128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split a text into sentences\ndef to_lines(text):\n    sents = text.strip().split('\\n')\n    sents = [i.split('\\t') for i in sents]\n    return sents","metadata":{"execution":{"iopub.status.busy":"2022-06-10T23:11:05.816794Z","iopub.execute_input":"2022-06-10T23:11:05.817551Z","iopub.status.idle":"2022-06-10T23:11:05.826713Z","shell.execute_reply.started":"2022-06-10T23:11:05.817504Z","shell.execute_reply":"2022-06-10T23:11:05.825416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ndata = read_text(\"../input/bilingual-sentence-pairs/deu.txt\")\ndeu_eng = pd.read_csv(\"../input/wikisql/validation.csv\")\n\n# joining with space content of text\n#text = ' '.join([i for i in text])\n\n# replacing ',' by space\n#text = text.replace(\",\", \"     \")\n#deu_eng = to_lines(data)\ndeu_eng = deu_eng.to_numpy()\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-10T23:11:05.857139Z","iopub.execute_input":"2022-06-10T23:11:05.857885Z","iopub.status.idle":"2022-06-10T23:11:06.21386Z","shell.execute_reply.started":"2022-06-10T23:11:05.857827Z","shell.execute_reply":"2022-06-10T23:11:06.212783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"deu_eng = deu_eng[:5000,:]","metadata":{"execution":{"iopub.status.busy":"2022-06-10T23:11:06.215514Z","iopub.execute_input":"2022-06-10T23:11:06.215871Z","iopub.status.idle":"2022-06-10T23:11:06.220607Z","shell.execute_reply.started":"2022-06-10T23:11:06.215839Z","shell.execute_reply":"2022-06-10T23:11:06.219429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<ul>\n    <li style=\"font-size:150%;\">The actual data contains over 150,000 sentence-pairs. However, we will use only the first 50,000 sentence pairs to reduce the training time of the model. You can change this number as per your system’s computation power.</li>\n</ul>","metadata":{}},{"cell_type":"markdown","source":"<h1> Text Cleaning / Preprocessing</h1>","metadata":{}},{"cell_type":"markdown","source":"<ul>\n    <li style=\"font-size:150%;\">We will get rid of the punctuation marks and then convert all the text to lower case.</li>\n</ul>","metadata":{}},{"cell_type":"code","source":"# Remove punctuation\ndeu_eng[:,0] = [s.translate(str.maketrans('', '', string.punctuation)) for s in deu_eng[:,0]]\ndeu_eng[:,1] = [s.translate(str.maketrans('', '', string.punctuation)) for s in deu_eng[:,1]]\n\ndeu_eng","metadata":{"execution":{"iopub.status.busy":"2022-06-10T23:11:06.222615Z","iopub.execute_input":"2022-06-10T23:11:06.222968Z","iopub.status.idle":"2022-06-10T23:11:06.286785Z","shell.execute_reply.started":"2022-06-10T23:11:06.222937Z","shell.execute_reply":"2022-06-10T23:11:06.285723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert text to lowercase\nfor i in range(len(deu_eng)):\n    deu_eng[i,0] = deu_eng[i,0].lower()\n    deu_eng[i,1] = deu_eng[i,1].lower()","metadata":{"execution":{"iopub.status.busy":"2022-06-10T23:11:06.288928Z","iopub.execute_input":"2022-06-10T23:11:06.289377Z","iopub.status.idle":"2022-06-10T23:11:06.302264Z","shell.execute_reply.started":"2022-06-10T23:11:06.289331Z","shell.execute_reply":"2022-06-10T23:11:06.301331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# empty lists\neng_l = []\ndeu_l = []\n\n# populate the lists with sentence lengths\nfor i in deu_eng[:,0]:\n      eng_l.append(len(i.split()))\n\nfor i in deu_eng[:,1]:\n      deu_l.append(len(i.split()))\n\nlength_df = pd.DataFrame({'eng':eng_l, 'deu':deu_l})\n\nlength_df.hist(bins = 30)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-10T23:11:06.303602Z","iopub.execute_input":"2022-06-10T23:11:06.304127Z","iopub.status.idle":"2022-06-10T23:11:06.696582Z","shell.execute_reply.started":"2022-06-10T23:11:06.304077Z","shell.execute_reply":"2022-06-10T23:11:06.695463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<ul>\n    <li style=\"font-size:150%;\">A Seq2Seq model requires that we convert both the input and the output sentences into integer sequences of fixed length.</li>\n    <li style=\"font-size:150%;\">Now, vectorize our text data by using Keras’s Tokenizer() class. It will turn our sentences into sequences of integers. We can then pad those sequences with zeros to make all the sequences of the same length.</li>\n    <li style=\"font-size:150%;\">Prepare tokenizers for both the German and English sentences</li>\n</ul>","metadata":{}},{"cell_type":"code","source":"# function to build a tokenizer\ndef tokenization(lines):\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(lines)\n    return tokenizer","metadata":{"execution":{"iopub.status.busy":"2022-06-10T23:11:06.698175Z","iopub.execute_input":"2022-06-10T23:11:06.698815Z","iopub.status.idle":"2022-06-10T23:11:06.704317Z","shell.execute_reply.started":"2022-06-10T23:11:06.698766Z","shell.execute_reply":"2022-06-10T23:11:06.703268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prepare english tokenizer\neng_tokenizer = tokenization(deu_eng[:, 0])\neng_vocab_size = len(eng_tokenizer.word_index) + 1\n\neng_length = 8\nprint('English Vocabulary Size: %d' % eng_vocab_size)","metadata":{"execution":{"iopub.status.busy":"2022-06-10T23:11:06.70582Z","iopub.execute_input":"2022-06-10T23:11:06.706449Z","iopub.status.idle":"2022-06-10T23:11:06.821569Z","shell.execute_reply.started":"2022-06-10T23:11:06.706396Z","shell.execute_reply":"2022-06-10T23:11:06.820338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prepare Deutch tokenizer\ndeu_tokenizer = tokenization(deu_eng[:, 1])\ndeu_vocab_size = len(deu_tokenizer.word_index) + 1\n\ndeu_length = 8\nprint('Deutch Vocabulary Size: %d' % deu_vocab_size)","metadata":{"execution":{"iopub.status.busy":"2022-06-10T23:11:06.825334Z","iopub.execute_input":"2022-06-10T23:11:06.825787Z","iopub.status.idle":"2022-06-10T23:11:06.928375Z","shell.execute_reply.started":"2022-06-10T23:11:06.825739Z","shell.execute_reply":"2022-06-10T23:11:06.92726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<ul>\n    <li style=\"font-size:150%;\">We tokenized the data — i.e., converted the text to numerical values. This allows the neural network to perform operations on the input data.</li>\n    <li style=\"font-size:150%;\">When we run the tokenizer, it creates a word index, which is then used to convert each sentence to a vector.</li>","metadata":{}},{"cell_type":"code","source":"# encode and pad sequences\ndef encode_sequences(tokenizer, length, lines):\n    seq = tokenizer.texts_to_sequences(lines)\n    # pad sequences with 0 values\n    seq = pad_sequences(seq, maxlen=length, padding='post')\n    return seq","metadata":{"execution":{"iopub.status.busy":"2022-06-10T23:11:06.930927Z","iopub.execute_input":"2022-06-10T23:11:06.931411Z","iopub.status.idle":"2022-06-10T23:11:06.936328Z","shell.execute_reply.started":"2022-06-10T23:11:06.931361Z","shell.execute_reply":"2022-06-10T23:11:06.93565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<ul>\n    <li style=\"font-size:150%;\">When we feed our sequences of word IDs into the model, each sequence needs to be the same length. To achieve this, padding is added to any sequence that is shorter than the max length (i.e. shorter than the longest sentence).</li>\n</ul>\n\n<center><img src=\"https://miro.medium.com/max/1728/0*6jZTOE0P7_i7N8pn.png\"></center>","metadata":{}},{"cell_type":"markdown","source":"<center><h1 style=\"font-size:200%; background-color:skyblue; color:black; padding:15px; font-family:Garamond;\"><b>Model Building</b></h1></center>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-size:150%;\">CODE REFERENCE: https://www.analyticsvidhya.com/blog/2019/01/neural-machine-translation-keras/</p>\n\n<p style=\"font-size:170%;\">First, let’s breakdown the architecture of an RNN at a high level. Referring to the diagram above, there are a few parts of the model we to be aware of:</p>\n\n<ul>\n    <li style=\"font-size:150%;\">Inputs. Input sequences are fed into the model with one word for every time step. Each word is encoded as a unique integer so that it maps to the German dataset vocabulary.</li>\n    <li style=\"font-size:150%;\">Embedding Layers. Embeddings are used to convert each word to a vector. The size of the vector depends on the complexity of the vocabulary.</li>\n    <li style=\"font-size:150%;\">LSTM Layer (Encoder). This is where the context from word vectors in previous time steps is applied to the current word vector.</li>\n    <li style=\"font-size:150%;\">Dense Layers (Decoder). These are typical fully connected layers used to decode the encoded input into the correct translation sequence.</li>\n    <li style=\"font-size:150%;\">he outputs are returned as a sequence of integers or one-hot encoded vectors which can then be mapped to the English dataset vocabulary.</li>\n</ul>\n    \n    \n<center><h1 style=\"font-size:150%;\">Model Architecture</h1></center>\n<center><img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2019/01/architecture.png\"></center>","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# split data into train and test set\ntrain, test = train_test_split(deu_eng, test_size=0.2, random_state = 12)","metadata":{"execution":{"iopub.status.busy":"2022-06-10T23:11:06.937534Z","iopub.execute_input":"2022-06-10T23:11:06.938018Z","iopub.status.idle":"2022-06-10T23:11:06.95125Z","shell.execute_reply.started":"2022-06-10T23:11:06.93798Z","shell.execute_reply":"2022-06-10T23:11:06.95016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<ul>\n    <li style=\"font-size:150%;\">It’s time to encode the sentences. We will encode German sentences as the input sequences and English sentences as the target sequences. This has to be done for both the train and test datasets.</li>","metadata":{}},{"cell_type":"code","source":"# prepare training data\ntrainX = encode_sequences(deu_tokenizer, deu_length, train[:, 1])\ntrainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n\n# prepare validation data\ntestX = encode_sequences(deu_tokenizer, deu_length, test[:, 1])\ntestY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])","metadata":{"execution":{"iopub.status.busy":"2022-06-10T23:11:06.952513Z","iopub.execute_input":"2022-06-10T23:11:06.952851Z","iopub.status.idle":"2022-06-10T23:11:07.192892Z","shell.execute_reply.started":"2022-06-10T23:11:06.952821Z","shell.execute_reply":"2022-06-10T23:11:07.191681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1> Define the Model</h1>","metadata":{}},{"cell_type":"code","source":"# build NMT model\ndef define_model(in_vocab,out_vocab, in_timesteps,out_timesteps,units):\n    model = Sequential()\n    model.add(Embedding(in_vocab, units, input_length=in_timesteps, mask_zero=True))\n    model.add(LSTM(units))\n    model.add(RepeatVector(out_timesteps))\n    model.add(LSTM(units, return_sequences=True))\n    model.add(Dense(out_vocab, activation='softmax'))\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-06-10T23:11:07.194852Z","iopub.execute_input":"2022-06-10T23:11:07.195327Z","iopub.status.idle":"2022-06-10T23:11:07.202126Z","shell.execute_reply.started":"2022-06-10T23:11:07.195279Z","shell.execute_reply":"2022-06-10T23:11:07.200978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model compilation\nmodel = define_model(deu_vocab_size, eng_vocab_size, deu_length, eng_length, 512)","metadata":{"execution":{"iopub.status.busy":"2022-06-10T23:11:07.203359Z","iopub.execute_input":"2022-06-10T23:11:07.203701Z","iopub.status.idle":"2022-06-10T23:11:08.868069Z","shell.execute_reply.started":"2022-06-10T23:11:07.203665Z","shell.execute_reply":"2022-06-10T23:11:08.867169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rms = optimizers.RMSprop(lr=0.001)\nmodel.compile(optimizer=rms, loss='sparse_categorical_crossentropy')","metadata":{"execution":{"iopub.status.busy":"2022-06-10T23:11:08.869412Z","iopub.execute_input":"2022-06-10T23:11:08.870052Z","iopub.status.idle":"2022-06-10T23:11:08.886371Z","shell.execute_reply.started":"2022-06-10T23:11:08.870006Z","shell.execute_reply":"2022-06-10T23:11:08.885547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<ul>\n    <li style=\"font-size:150%;\">We are using the RMSprop optimizer in this model as it’s usually a good choice when working with recurrent neural networks.</li>\n    <li style=\"font-size:150%;\">Here I have used ‘sparse_categorical_crossentropy‘ as the loss function. This is because the function allows us to use the target sequence as is, instead of the one-hot encoded format. One-hot encoding the target sequences using such a huge vocabulary might consume our system’s entire memory.</li>\n               \n</ul>","metadata":{}},{"cell_type":"markdown","source":"<h1> Fit the Model</h1>","metadata":{}},{"cell_type":"markdown","source":"<ul>\n    <li style=\"font-size:150%;\">ModelCheckpoint() function to save the model with the lowest validation loss. I personally prefer this method over early stopping.</li>\n</ul>","metadata":{}},{"cell_type":"code","source":"filename = 'model.h1.24_jan_19'\ncheckpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n\n# train model\nhistory = model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1], 1),\n                    epochs=30, batch_size=512, validation_split = 0.2,callbacks=[checkpoint], \n                    verbose=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-10T23:11:08.887604Z","iopub.execute_input":"2022-06-10T23:11:08.887938Z","iopub.status.idle":"2022-06-10T23:19:21.375503Z","shell.execute_reply.started":"2022-06-10T23:11:08.887906Z","shell.execute_reply":"2022-06-10T23:19:21.374735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.legend(['train','validation'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-10T23:19:21.377486Z","iopub.execute_input":"2022-06-10T23:19:21.3781Z","iopub.status.idle":"2022-06-10T23:19:21.552409Z","shell.execute_reply.started":"2022-06-10T23:19:21.378062Z","shell.execute_reply":"2022-06-10T23:19:21.551549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1> Prediction on unseen data</h1>","metadata":{}},{"cell_type":"code","source":"model = load_model('model.h1.24_jan_19')\npreds = model.predict_classes(testX.reshape((testX.shape[0],testX.shape[1])))","metadata":{"execution":{"iopub.status.busy":"2022-06-10T23:19:21.553831Z","iopub.execute_input":"2022-06-10T23:19:21.554147Z","iopub.status.idle":"2022-06-10T23:19:35.315997Z","shell.execute_reply.started":"2022-06-10T23:19:21.554117Z","shell.execute_reply":"2022-06-10T23:19:35.315136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_word(n, tokenizer):\n    for word, index in tokenizer.word_index.items():\n        if index == n:\n            return word\n    return None","metadata":{"execution":{"iopub.status.busy":"2022-06-10T23:19:35.317894Z","iopub.execute_input":"2022-06-10T23:19:35.318335Z","iopub.status.idle":"2022-06-10T23:19:35.326951Z","shell.execute_reply.started":"2022-06-10T23:19:35.318289Z","shell.execute_reply":"2022-06-10T23:19:35.325825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_text = []\nfor i in preds:\n    temp = []\n    for j in range(len(i)):\n        t = get_word(i[j], eng_tokenizer)\n        if j > 0:\n            if (t == get_word(i[j-1], eng_tokenizer)) or (t == None):\n                temp.append('')\n            else:\n                temp.append(t)\n        else:\n            if(t == None):\n                temp.append('')\n            else:\n                temp.append(t) \n\n    preds_text.append(' '.join(temp))","metadata":{"execution":{"iopub.status.busy":"2022-06-10T23:19:35.32884Z","iopub.execute_input":"2022-06-10T23:19:35.329362Z","iopub.status.idle":"2022-06-10T23:19:47.22955Z","shell.execute_reply.started":"2022-06-10T23:19:35.329309Z","shell.execute_reply":"2022-06-10T23:19:47.228741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_df = pd.DataFrame({'actual' : test[:,0], 'predicted' : preds_text})","metadata":{"execution":{"iopub.status.busy":"2022-06-10T23:19:47.230885Z","iopub.execute_input":"2022-06-10T23:19:47.231386Z","iopub.status.idle":"2022-06-10T23:19:47.237775Z","shell.execute_reply.started":"2022-06-10T23:19:47.231353Z","shell.execute_reply":"2022-06-10T23:19:47.236616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print 15 rows randomly\npred_df.head(15)","metadata":{"execution":{"iopub.status.busy":"2022-06-10T23:19:47.23922Z","iopub.execute_input":"2022-06-10T23:19:47.239546Z","iopub.status.idle":"2022-06-10T23:19:47.256817Z","shell.execute_reply.started":"2022-06-10T23:19:47.239515Z","shell.execute_reply":"2022-06-10T23:19:47.255943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<center><h1> CONCLUSION</h1></center>\n<br>\n\n<ul>\n    <li style=\"font-size:150%;\">Our Seq2Seq model does a decent job. But there are several instances where it misses out on understanding the key words.</li>\n    <li style=\"font-size:150%;\">These are the challenges you will face on a regular basis in NLP. But these aren’t immovable obstacles. We can mitigate such challenges by using more training data and building a better (or more complex) model.</li>\n</ul>","metadata":{}},{"cell_type":"code","source":"!pip install sacrebleu","metadata":{"execution":{"iopub.status.busy":"2022-06-10T23:19:47.258147Z","iopub.execute_input":"2022-06-10T23:19:47.258823Z","iopub.status.idle":"2022-06-10T23:19:54.156945Z","shell.execute_reply.started":"2022-06-10T23:19:47.258789Z","shell.execute_reply":"2022-06-10T23:19:54.155642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n\n\n\nimport numpy as np\n\n# 2d array to list\n\narr=test[:,0] \n\n\nrefrrr = arr.tolist()\n\n\n# arr2=preds_text\n# hypp=arr2.tolist()","metadata":{"execution":{"iopub.status.busy":"2022-06-10T23:19:54.161411Z","iopub.execute_input":"2022-06-10T23:19:54.161763Z","iopub.status.idle":"2022-06-10T23:19:54.168744Z","shell.execute_reply.started":"2022-06-10T23:19:54.161729Z","shell.execute_reply":"2022-06-10T23:19:54.167436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x=type(preds_text)\nx\n","metadata":{"execution":{"iopub.status.busy":"2022-06-10T23:19:54.17123Z","iopub.execute_input":"2022-06-10T23:19:54.171798Z","iopub.status.idle":"2022-06-10T23:19:54.186658Z","shell.execute_reply.started":"2022-06-10T23:19:54.171668Z","shell.execute_reply":"2022-06-10T23:19:54.185449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y=type(refrrr)\ny","metadata":{"execution":{"iopub.status.busy":"2022-06-10T23:19:54.188098Z","iopub.execute_input":"2022-06-10T23:19:54.188479Z","iopub.status.idle":"2022-06-10T23:19:54.199003Z","shell.execute_reply.started":"2022-06-10T23:19:54.188446Z","shell.execute_reply":"2022-06-10T23:19:54.198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sacrebleu.metrics import BLEU\n\n# refs = [['The dog bit the guy.', 'It was not unexpected.', 'The man bit him first.'],\n#         ['The dog had bit the man.', 'No one was surprised.', 'The man had bitten the dog.']]\n\n# hyps = ['The dog bit the man.', \"It wasn't surprising.\", 'The man had just bitten him.']\n\n\nrefs = refrrr\nhyps = preds_text\n\nbleu = BLEU()\n\nresult = bleu.corpus_score(hyps, refs)\nprint(result)\n# BLEU = 29.44 82.4/42.9/27.3/12.5 (BP = 0.889 ratio = 0.895 hyp_len = 17 ref_len = 19)","metadata":{"execution":{"iopub.status.busy":"2022-06-10T23:19:54.201972Z","iopub.execute_input":"2022-06-10T23:19:54.202484Z","iopub.status.idle":"2022-06-10T23:19:54.408456Z","shell.execute_reply.started":"2022-06-10T23:19:54.202435Z","shell.execute_reply":"2022-06-10T23:19:54.407393Z"},"trusted":true},"execution_count":null,"outputs":[]}]}