{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30028,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h2><center>Translating Text to SQL with T5 Transformer</center></h2>\n\n![](https://i.imgur.com/jVFMMWR.png)\n\n<h4><center>Image Source:  <a href=\"https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html\">Google AI Blog</a></center></h4>","metadata":{}},{"cell_type":"markdown","source":"### Install Transformers Datasets (to get [wikiSQL dataset](https://huggingface.co/nlp/viewer/?dataset=wikisql))","metadata":{}},{"cell_type":"code","source":"!pip install -q -U datasets > /dev/null","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Libraries ðŸ“šâ¬‡","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelWithLMHead, AutoTokenizer\nfrom datasets import load_dataset\nimport random, warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Import the T5-base model [fine-tuned on WikiSQL](https://huggingface.co/mrm8488/t5-base-finetuned-wikiSQL?text=My+name+is+Wolfgang+and+I+live+in+Berlin) from [ðŸ¤—/transformers](https://github.com/huggingface/transformers) [thanks to [Manuel Romero](https://huggingface.co/mrm8488)]","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-wikiSQL\")\nmodel = AutoModelWithLMHead.from_pretrained(\"mrm8488/t5-base-finetuned-wikiSQL\")","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Predict Function","metadata":{}},{"cell_type":"code","source":"def get_sql(query):\n    \n    input_text = \"translate English to SQL: %s </s>\" % query\n    \n    features = tokenizer([input_text], return_tensors='pt')\n\n    output = model.generate(input_ids=features['input_ids'], \n               attention_mask=features['attention_mask'])\n\n    return tokenizer.decode(output[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_dataset = load_dataset('wikisql', split='validation')","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Sample Validation Data","metadata":{}},{"cell_type":"code","source":"valid_dataset[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Prediction on WikiSQL Validation Set","metadata":{}},{"cell_type":"code","source":"for idx in random.sample(range(len(valid_dataset)), 250):\n    print(f\"Text: {valid_dataset[idx]['question']}\")\n    print(f\"Pred SQL: {get_sql(valid_dataset[idx]['question'])}\")\n    print(f\"True SQL: {valid_dataset[idx]['sql']['human_readable']}\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}